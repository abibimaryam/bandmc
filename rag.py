# import os
# import re
# import pandas as pd
# from langchain_chroma import Chroma
# from langchain.prompts import ChatPromptTemplate
# from langchain_community.llms.ollama import Ollama
# from embedding_functiom import embedding_function
# from config import CHROMA_PATH, model
# from ddgs import DDGS
# from docx import Document
# import pdfplumber
# import time


# SUPPORTED_EXTENSIONS = [".txt", ".pdf", ".docx", ".csv", ".xlsx"]

# PROMPT_TEMPLATE = """
# –ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥—ë–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:

# {context}

# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–¥–µ–ª–∞–ª —Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ:

# {question}

# –í —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ —É–∫–∞–∑–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {key_entities}

# –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –Ω–∞–π–¥–∏ –í–°–ï –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ –æ—à–∏–±–∫–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–æ–≤–µ—Ä—å, —Å–æ–≤–ø–∞–¥–∞—é—Ç –ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å –∏ —Å—Ç—Ä–∞–Ω–∞-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.

# –í—ã–≤–µ–¥–∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–ª—å–∫–æ —Ç–µ—Ö –ø—É–Ω–∫—Ç–æ–≤, –≥–¥–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ù–ï –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢ –¥–∞–Ω–Ω—ã–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å –∫—Ä–∞—Ç–∫–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º.

# –ï—Å–ª–∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –Ω–µ—Ç, –æ—Ç–≤–µ—Ç—å "–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
# """

# # --- –í–µ–±-–ø–æ–∏—Å–∫
# def search_tool(query: str) -> str:
#     with DDGS() as ddgs:
#         results = [r["body"] for r in ddgs.text(query, max_results=3)]
#     return "\n\n".join(results)


# def extract_key_entities(user_input: str) -> str:
#     prompt = f"""
# –ò–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, —Ç–∞–∫–∏–µ –∫–∞–∫:

# - —Ç–æ—Ä–≥–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞,
# - –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –Ω–µ–ø–∞—Ç–µ–Ω—Ç–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å),

# –í–µ—Ä–Ω–∏ –∏—Ö —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é ‚Äî –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–µ—Ä–Ω–∏ "None".

# –ó–∞–ø—Ä–æ—Å: "{user_input}"
# """
#     response = model.invoke(prompt).strip()
#     print(f"üîë –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {response}")
#     return response if response.lower() != "none" else None


# def query_rag_with_web_search(user_input: str):
#     key_phrases = extract_key_entities(user_input)
#     query_text = key_phrases or user_input
#     filter_name = key_phrases.split(",")[0].strip().lower() if key_phrases else None

#     db = Chroma(
#         collection_name="meds",
#         persist_directory=CHROMA_PATH,
#         embedding_function=embedding_function()
#     )
#     results = db.similarity_search_with_score(query_text, k=20)

#     if filter_name:
#         results = [(doc, score) for doc, score in results
#                    if filter_name in doc.metadata.get("—Ç–æ—Ä–≥–æ–≤–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ", "").lower()]

#     local_context = "\n\n---\n\n".join([doc.page_content for doc, _ in results]) if results else ""
#     web_results = search_tool(user_input)

#     combined_context = f"–õ–æ–∫–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö:\n{local_context}\n\n–ò–Ω—Ç–µ—Ä–Ω–µ—Ç –¥–∞–Ω–Ω—ã–µ:\n{web_results}"

#     prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
#     prompt = prompt_template.format(
#         context=combined_context,
#         question=user_input,
#         key_entities=key_phrases or "None"
#     )

#     response_text = model.invoke(prompt).strip()
#     return response_text


# # --- –§—É–Ω–∫—Ü–∏–∏ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
# def read_txt(filepath: str) -> str:
#     with open(filepath, "r", encoding="utf-8") as f:
#         return f.read()


# def read_docx(filepath: str) -> str:
#     doc = Document(filepath)
#     return "\n".join([p.text for p in doc.paragraphs])


# def read_pdf(filepath: str) -> str:
#     text = ""
#     with pdfplumber.open(filepath) as pdf:
#         for page in pdf.pages:
#             text += page.extract_text() + "\n"
#     return text


# def read_excel_csv(filepath: str) -> str:
#     ext = os.path.splitext(filepath)[-1].lower()
#     df = pd.read_excel(filepath, header=0) if ext == ".xlsx" else pd.read_csv(filepath, header=0)

#     print("üìã –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã:", df.columns.tolist())

#     # –î–æ–±–∞–≤–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ
#     rows = []
#     for _, row in df.iterrows():
#         row_str = ", ".join([f"{col.strip()}: {str(val).strip()}" for col, val in row.items()])
#         rows.append(row_str)

#     return "\n".join(rows)



# # --- –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ —Ç–µ–∫—Å—Ç–∞
# def extract_text_from_file(filepath: str) -> str:
#     ext = os.path.splitext(filepath)[-1].lower()
#     if ext == ".txt":
#         return read_txt(filepath)
#     elif ext == ".docx":
#         return read_docx(filepath)
#     elif ext == ".pdf":
#         return read_pdf(filepath)
#     elif ext in [".csv", ".xlsx"]:
#         return read_excel_csv(filepath)
#     else:
#         raise ValueError(f"‚ùå –§–æ—Ä–º–∞—Ç {ext} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: {', '.join(SUPPORTED_EXTENSIONS)}")


# def split_document_into_statements(text: str) -> list:
#     # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–æ–∫–∞–º –∏–ª–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
#     return [s.strip() for s in re.split(r'\n+|(?<=[.!?])\s+', text) if s.strip()]




# def query_with_retries(statement: str, retries: int = 2, delay: float = 2.0) -> str:
#     for attempt in range(1, retries + 1):
#         try:
#             return query_rag_with_web_search(statement)
#         except Exception as e:
#             print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{retries}): {e}")
#             if attempt < retries:
#                 print(f"‚è≥ –ñ–¥—ë–º {delay} —Å–µ–∫ –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º...")
#                 time.sleep(delay)
#             else:
#                 return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫: {e}"

# # --- –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
# def process_document(filepath: str):
#     if not os.path.exists(filepath):
#         print(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
#         return

#     print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {filepath}")
#     try:
#         text = extract_text_from_file(filepath)
#     except Exception as e:
#         print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
#         return

#     statements = split_document_into_statements(text)
#     print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(statements)} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π.\n")

#     all_results = []
#     for i, statement in enumerate(statements, start=1):
#         print(f"\nüîç –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ {i}:\n{statement}")
#         result = query_with_retries(statement, retries=2, delay=2)
#         all_results.append((statement, result))

#     print("\nüßæ –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç –ø–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è–º:\n")
#     for i, (stmt, mismatch) in enumerate(all_results, start=1):
#         print(f"{i}. üìå {stmt}")
#         print(f"   ‚ùó –ü—Ä–æ–≤–µ—Ä–∫–∞: {mismatch}")
#         print("------")

# # --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
# if __name__ == "__main__":
#     FILEPATH = "/home/maryam/–î–æ–∫—É–º–µ–Ω—Ç—ã/projects/bandmc/test_data/test.xlsx"  # –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
#     process_document(FILEPATH)


# import os
# import re
# import time
# import pandas as pd
# from docx import Document
# import pdfplumber
# from ddgs import DDGS
# from langchain_chroma import Chroma
# from langchain.prompts import ChatPromptTemplate
# from langchain_community.llms.ollama import Ollama
# from embedding_functiom import embedding_function
# from config import CHROMA_PATH, model

# # SUPPORTED_EXTENSIONS = [".txt", ".pdf", ".docx", ".csv", ".xlsx"]

# # PROMPT_TEMPLATE = """
# # –ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥—ë–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:

# # {context}

# # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–¥–µ–ª–∞–ª —Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ:

# # {question}

# # –í —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ —É–∫–∞–∑–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {key_entities}

# # –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –Ω–∞–π–¥–∏ –í–°–ï –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ –æ—à–∏–±–∫–∏.

# # –û—Å–æ–±–µ–Ω–Ω–æ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—å:
# # - –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ —Ç–æ—Ä–≥–æ–≤–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ;
# # - –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –Ω–µ–ø–∞—Ç–µ–Ω—Ç–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ;
# # - –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å;
# # - –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ —Å—Ç—Ä–∞–Ω–∞-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å.

# # –ï—Å–ª–∏ –∫–∞–∫–æ–µ-–ª–∏–±–æ –∏–∑ —ç—Ç–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä—É–π —ç—Ç–æ.

# # –í—ã–≤–µ–¥–∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–ª—å–∫–æ —Ç–µ—Ö –ø—É–Ω–∫—Ç–æ–≤, –≥–¥–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ù–ï –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢ –¥–∞–Ω–Ω—ã–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å –∫—Ä–∞—Ç–∫–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º.

# # –ï—Å–ª–∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –Ω–µ—Ç, –æ—Ç–≤–µ—Ç—å "–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
# # """

# # # --- –í–µ–±-–ø–æ–∏—Å–∫
# # def search_tool(query: str, retries=3, delay=2) -> str:
# #     for attempt in range(retries):
# #         try:
# #             with DDGS() as ddgs:
# #                 results = [r["body"] for r in ddgs.text(query, max_results=3)]
# #             return "\n\n".join(results)
# #         except Exception as e:
# #             print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞: {e}")
# #             if attempt < retries - 1:
# #                 print(f"‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫...")
# #                 time.sleep(delay)
# #     return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞."

# # # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
# # def extract_key_entities(user_input: str) -> str:
# #     prompt = f"""
# # –ò–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, —Ç–∞–∫–∏–µ –∫–∞–∫:

# # - —Ç–æ—Ä–≥–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞,
# # - –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –Ω–µ–ø–∞—Ç–µ–Ω—Ç–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å),

# # –í–µ—Ä–Ω–∏ –∏—Ö —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é ‚Äî –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–µ—Ä–Ω–∏ "None".

# # –ó–∞–ø—Ä–æ—Å: "{user_input}"
# # """
# #     response = model.invoke(prompt).strip()
# #     print(f"üîë –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {response}")
# #     return response if response.lower() != "none" else None

# # # --- –ü–æ–∏—Å–∫ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
# # def query_rag_with_web_search(user_input: str):
# #     key_phrases = extract_key_entities(user_input)
# #     query_text = key_phrases or user_input
# #     filter_name = key_phrases.split(",")[0].strip().lower() if key_phrases else None

# #     db = Chroma(
# #         collection_name="meds",
# #         persist_directory=CHROMA_PATH,
# #         embedding_function=embedding_function()
# #     )
# #     results = db.similarity_search_with_score(query_text, k=20)

# #     if filter_name:
# #         results = [(doc, score) for doc, score in results
# #                    if filter_name in doc.metadata.get("—Ç–æ—Ä–≥–æ–≤–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ", "").lower()]

# #     if not results:
# #         print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É '{filter_name}'")
# #         local_context = ""
# #     else:
# #         print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É '{filter_name}'")
# #         print("üîç –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
# #         doc = results[0][0]
# #         print(doc.page_content[:500])
# #         print("üìå –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:", doc.metadata)
# #         local_context = "\n\n---\n\n".join([doc.page_content for doc, _ in results])

# #     print("üåê –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–µ–±-–ø–æ–∏—Å–∫...")
# #     web_results = search_tool(user_input)
# #     print("üåê Web-–∫–æ–Ω—Ç–µ–∫—Å—Ç:\n", web_results[:300])

# #     combined_context = f"–õ–æ–∫–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö:\n{local_context}\n\n–ò–Ω—Ç–µ—Ä–Ω–µ—Ç –¥–∞–Ω–Ω—ã–µ:\n{web_results}"

# #     prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
# #     prompt = prompt_template.format(
# #         context=combined_context,
# #         question=user_input,
# #         key_entities=key_phrases or "None"
# #     )

# #     response_text = model.invoke(prompt).strip()
# #     return response_text

# # # --- –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
# # def read_txt(filepath: str) -> str:
# #     with open(filepath, "r", encoding="utf-8") as f:
# #         return f.read()

# # def read_docx(filepath: str) -> str:
# #     doc = Document(filepath)
# #     return "\n".join([p.text for p in doc.paragraphs])

# # def read_pdf(filepath: str) -> str:
# #     text = ""
# #     with pdfplumber.open(filepath) as pdf:
# #         for page in pdf.pages:
# #             page_text = page.extract_text()
# #             if page_text:
# #                 text += page_text + "\n"
# #     return text

# # def read_excel_csv(filepath: str) -> str:
# #     ext = os.path.splitext(filepath)[-1].lower()
# #     df = pd.read_excel(filepath, header=0) if ext == ".xlsx" else pd.read_csv(filepath, header=0)
# #     rows = []
# #     for _, row in df.iterrows():
# #         row_str = ", ".join([f"{col.strip()}: {str(val).strip()}" for col, val in row.items()])
# #         rows.append(row_str)
# #     return "\n".join(rows)

# # def extract_text_from_file(filepath: str) -> str:
# #     ext = os.path.splitext(filepath)[-1].lower()
# #     if ext == ".txt":
# #         return read_txt(filepath)
# #     elif ext == ".docx":
# #         return read_docx(filepath)
# #     elif ext == ".pdf":
# #         return read_pdf(filepath)
# #     elif ext in [".csv", ".xlsx"]:
# #         return read_excel_csv(filepath)
# #     else:
# #         raise ValueError(f"‚ùå –§–æ—Ä–º–∞—Ç {ext} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: {', '.join(SUPPORTED_EXTENSIONS)}")

# # def split_document_into_statements(text: str) -> list:
# #     return [s.strip() for s in re.split(r'\n+|(?<=[.!?])\s+', text) if s.strip()]

# # # --- –ü–æ–ø—ã—Ç–∫–∏ —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏
# # def query_with_retries(statement: str, retries: int = 2, delay: float = 2.0) -> str:
# #     for attempt in range(1, retries + 1):
# #         try:
# #             return query_rag_with_web_search(statement)
# #         except Exception as e:
# #             print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{retries}): {e}")
# #             if attempt < retries:
# #                 print(f"‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫...")
# #                 time.sleep(delay)
# #     return f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫."

# # # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
# # def process_document(filepath: str):
# #     if not os.path.exists(filepath):
# #         print(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
# #         return

# #     print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {filepath}")
# #     try:
# #         text = extract_text_from_file(filepath)
# #     except Exception as e:
# #         print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
# #         return

# #     statements = split_document_into_statements(text)
# #     print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(statements)} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π.\n")

# #     all_results = []
# #     for i, statement in enumerate(statements, start=1):
# #         print(f"\nüîç –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ {i}:\n{statement}")
# #         result = query_with_retries(statement)
# #         all_results.append((statement, result))

# #     print("\nüßæ –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç –ø–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è–º:\n")
# #     for i, (stmt, mismatch) in enumerate(all_results, start=1):
# #         print(f"{i}. üìå {stmt}")
# #         print(f"   ‚ùó –ü—Ä–æ–≤–µ—Ä–∫–∞: {mismatch}")
# #         print("------")

# # # --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
# # if __name__ == "__main__":
# #     FILEPATH = "/home/maryam/–î–æ–∫—É–º–µ–Ω—Ç—ã/projects/bandmc/test_data/test.xlsx"  # üîÅ –ü–æ–¥—Å—Ç–∞–≤—å —Å–≤–æ–π –ø—É—Ç—å
# #     process_document(FILEPATH)

import os
import re
import time
import pandas as pd
from docx import Document
import pdfplumber
from ddgs import DDGS
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from embedding_functiom import embedding_function
from config import CHROMA_PATH, model

SUPPORTED_EXTENSIONS = [".txt", ".pdf", ".docx", ".csv", ".xlsx"]

PROMPT_TEMPLATE = """
–ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥—ë–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:

{context}

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–¥–µ–ª–∞–ª —Å–ª–µ–¥—É—é—â–µ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ:

{question}

–í —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏ —É–∫–∞–∑–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {key_entities}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ –Ω–∞–π–¥–∏ –í–°–ï –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ –æ—à–∏–±–∫–∏.

–û—Å–æ–±–µ–Ω–Ω–æ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—å:
- –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ —Ç–æ—Ä–≥–æ–≤–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ;
- –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –Ω–µ–ø–∞—Ç–µ–Ω—Ç–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ;
- –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å;
- –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ —Å—Ç—Ä–∞–Ω–∞-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å.

–ï—Å–ª–∏ –∫–∞–∫–æ–µ-–ª–∏–±–æ –∏–∑ —ç—Ç–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä—É–π —ç—Ç–æ.

–í—ã–≤–µ–¥–∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–ª—å–∫–æ —Ç–µ—Ö –ø—É–Ω–∫—Ç–æ–≤, –≥–¥–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ù–ï –°–û–û–¢–í–ï–¢–°–¢–í–£–ï–¢ –¥–∞–Ω–Ω—ã–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å –∫—Ä–∞—Ç–∫–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º.

–ï—Å–ª–∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –Ω–µ—Ç, –æ—Ç–≤–µ—Ç—å "–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
"""

# --- –í–µ–±-–ø–æ–∏—Å–∫
def search_tool(query: str, retries=3, delay=2) -> str:
    for attempt in range(retries):
        try:
            with DDGS() as ddgs:
                results = [r["body"] for r in ddgs.text(query, max_results=3)]
            return "\n\n".join(results)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞: {e}")
            if attempt < retries - 1:
                print(f"‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫...")
                time.sleep(delay)
    return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞."

# --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
def extract_key_entities(user_input: str) -> str:
    prompt = f"""
–ò–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, —Ç–∞–∫–∏–µ –∫–∞–∫:

- —Ç–æ—Ä–≥–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞,
- –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –Ω–µ–ø–∞—Ç–µ–Ω—Ç–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å),

–í–µ—Ä–Ω–∏ –∏—Ö —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é ‚Äî –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–µ—Ä–Ω–∏ "None".

–ó–∞–ø—Ä–æ—Å: "{user_input}"
"""
    response = model.invoke(prompt)
    response_text = response.content.strip()
    print(f"üîë –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {response_text}")
    return response_text if response_text.lower() != "none" else None

# --- –ü–æ–∏—Å–∫ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
def query_rag_with_web_search(user_input: str):
    key_phrases = extract_key_entities(user_input)
    query_text = key_phrases or user_input
    filter_name = key_phrases.split(",")[0].strip().lower() if key_phrases else None

    db = Chroma(
        collection_name="meds",
        persist_directory=CHROMA_PATH,
        embedding_function=embedding_function()
    )
    results = db.similarity_search_with_score(query_text, k=20)

    if filter_name:
        results = [(doc, score) for doc, score in results
                   if filter_name in doc.metadata.get("—Ç–æ—Ä–≥–æ–≤–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ", "").lower()]

    if not results:
        print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É '{filter_name}'")
        local_context = ""
    else:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É '{filter_name}'")
        print("üîç –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
        doc = results[0][0]
        print(doc.page_content[:500])
        print("üìå –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:", doc.metadata)
        local_context = "\n\n---\n\n".join([doc.page_content for doc, _ in results])

    print("üåê –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–µ–±-–ø–æ–∏—Å–∫...")
    web_results = search_tool(user_input)
    print("üåê Web-–∫–æ–Ω—Ç–µ–∫—Å—Ç:\n", web_results[:300])

    combined_context = f"–õ–æ–∫–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö:\n{local_context}\n\n–ò–Ω—Ç–µ—Ä–Ω–µ—Ç –¥–∞–Ω–Ω—ã–µ:\n{web_results}"

    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(
        context=combined_context,
        question=user_input,
        key_entities=key_phrases or "None"
    )

    response = model.invoke(prompt)
    response_text = response.content.strip()
    return response_text

# --- –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
def read_txt(filepath: str) -> str:
    with open(filepath, "r", encoding="utf-8") as f:
        return f.read()

def read_docx(filepath: str) -> str:
    doc = Document(filepath)
    return "\n".join([p.text for p in doc.paragraphs])

def read_pdf(filepath: str) -> str:
    text = ""
    with pdfplumber.open(filepath) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

def read_excel_csv(filepath: str) -> str:
    ext = os.path.splitext(filepath)[-1].lower()
    df = pd.read_excel(filepath, header=0) if ext == ".xlsx" else pd.read_csv(filepath, header=0)
    rows = []
    for _, row in df.iterrows():
        row_str = ", ".join([f"{col.strip()}: {str(val).strip()}" for col, val in row.items()])
        rows.append(row_str)
    return "\n".join(rows)

def extract_text_from_file(filepath: str) -> str:
    ext = os.path.splitext(filepath)[-1].lower()
    if ext == ".txt":
        return read_txt(filepath)
    elif ext == ".docx":
        return read_docx(filepath)
    elif ext == ".pdf":
        return read_pdf(filepath)
    elif ext in [".csv", ".xlsx"]:
        return read_excel_csv(filepath)
    else:
        raise ValueError(f"‚ùå –§–æ—Ä–º–∞—Ç {ext} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: {', '.join(SUPPORTED_EXTENSIONS)}")

def split_document_into_statements(text: str) -> list:
    return [s.strip() for s in re.split(r'\n+|(?<=[.!?])\s+', text) if s.strip()]

# --- –ü–æ–ø—ã—Ç–∫–∏ —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏
def query_with_retries(statement: str, retries: int = 2, delay: float = 2.0) -> str:
    for attempt in range(1, retries + 1):
        try:
            return query_rag_with_web_search(statement)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{retries}): {e}")
            if attempt < retries:
                print(f"‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫...")
                time.sleep(delay)
    return f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫."

# --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
def process_document(filepath: str):
    if not os.path.exists(filepath):
        print(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {filepath}")
    try:
        text = extract_text_from_file(filepath)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return

    statements = split_document_into_statements(text)
    print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(statements)} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π.\n")

    all_results = []
    for i, statement in enumerate(statements, start=1):
        print(f"\nüîç –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ {i}:\n{statement}")
        result = query_with_retries(statement)
        all_results.append((statement, result))

    print("\nüßæ –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç –ø–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è–º:\n")
    for i, (stmt, mismatch) in enumerate(all_results, start=1):
        print(f"{i}. üìå {stmt}")
        print(f"   ‚ùó –ü—Ä–æ–≤–µ—Ä–∫–∞: {mismatch}")
        print("------")

# --- –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    FILEPATH = "/home/maryam/–î–æ–∫—É–º–µ–Ω—Ç—ã/projects/bandmc/test_data/test.xlsx"  # üîÅ –ü–æ–¥—Å—Ç–∞–≤—å —Å–≤–æ–π –ø—É—Ç—å
    process_document(FILEPATH)
